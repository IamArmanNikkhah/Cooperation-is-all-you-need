{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import all dependencies ðŸ˜ŠðŸ‘Œ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\nimport torchvision.datasets as datasets # Has standard datasets we can import in a nice way\nimport torchvision.transforms as transforms # Transformations we can perform on our dataset\nimport torch.nn.functional as F # All functions that don't have any parameters\nfrom torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\nfrom torchvision.datasets import ImageFolder\nimport torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\nfrom PIL import Image\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-22T20:34:08.154396Z","iopub.execute_input":"2021-08-22T20:34:08.154729Z","iopub.status.idle":"2021-08-22T20:34:09.62256Z","shell.execute_reply.started":"2021-08-22T20:34:08.154695Z","shell.execute_reply":"2021-08-22T20:34:09.62184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set device ðŸŽ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu or cpu\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:34:13.26399Z","iopub.execute_input":"2021-08-22T20:34:13.264326Z","iopub.status.idle":"2021-08-22T20:34:13.329278Z","shell.execute_reply.started":"2021-08-22T20:34:13.264296Z","shell.execute_reply":"2021-08-22T20:34:13.328158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing data set âœ”","metadata":{}},{"cell_type":"markdown","source":"**Define data set class ðŸ¤·â€â™‚ï¸**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndataset = ImageFolder(\"../input/cat-and-dog/training_set/training_set/\")\ntrain_data, test_data, train_label, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.2, random_state=42)\n\n# ImageLoader Class\n\nclass ImageLoader(Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = self.checkChannel(dataset) # some images are CMYK, Grayscale, check only RGB \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, item):\n        image = Image.open(self.dataset[item][0])\n        classCategory = self.dataset[item][1]\n        if self.transform:\n            image = self.transform(image)\n        return image, classCategory\n        \n    \n    def checkChannel(self, dataset):\n        datasetRGB = []\n        for index in range(len(dataset)):\n            if (Image.open(dataset[index][0]).getbands() == (\"R\", \"G\", \"B\")): # Check Channels\n                datasetRGB.append(dataset[index])\n        return datasetRGB","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-22T20:34:22.688477Z","iopub.execute_input":"2021-08-22T20:34:22.688823Z","iopub.status.idle":"2021-08-22T20:34:26.942516Z","shell.execute_reply.started":"2021-08-22T20:34:22.688775Z","shell.execute_reply":"2021-08-22T20:34:26.941561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image transformation: Resizing normalizing ðŸ˜Ž**","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0]*1, [1]*1)\n]) # train transform\n\ntest_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0]*1, [1]*1)\n]) # test transform\n\ntrain_dataset = ImageLoader(train_data, train_transform)\ntest_dataset = ImageLoader(test_data, test_transform)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:34:46.500421Z","iopub.execute_input":"2021-08-22T20:34:46.500763Z","iopub.status.idle":"2021-08-22T20:35:13.383552Z","shell.execute_reply.started":"2021-08-22T20:34:46.50073Z","shell.execute_reply":"2021-08-22T20:35:13.382716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define data set loaderâš™**","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:35:18.7085Z","iopub.execute_input":"2021-08-22T20:35:18.708877Z","iopub.status.idle":"2021-08-22T20:35:18.713877Z","shell.execute_reply.started":"2021-08-22T20:35:18.708839Z","shell.execute_reply":"2021-08-22T20:35:18.712776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading resnet pre train network ðŸ±â€ðŸ**","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torchvision import models\n\n# load pretrain model and modify...\nmodel = models.resnet50(pretrained=True)\n\nmodel.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\n\nmodel.to(device)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-22T20:35:32.32065Z","iopub.execute_input":"2021-08-22T20:35:32.321024Z","iopub.status.idle":"2021-08-22T20:35:43.594456Z","shell.execute_reply.started":"2021-08-22T20:35:32.320991Z","shell.execute_reply":"2021-08-22T20:35:43.593779Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the greatest neural network ever by armanðŸ‘¨â€ðŸ”¬","metadata":{}},{"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n# Train and test\n\ndef train(num_epoch, model):\n    for epoch in range(0, num_epoch):\n\n        losses = []\n        model.train()\n        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n        for batch_idx, (data, targets) in loop:\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            scores = model(data)\n            \n            loss = criterion(scores, targets)\n            optimizer.zero_grad()\n            losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            _, preds = torch.max(scores, 1)\n            \n            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n            loop.set_postfix(loss=loss.data.item())\n        \n        # save model\n        torch.save({ \n                    'model_state_dict': model.state_dict(), \n                    'optimizer_state_dict': optimizer.state_dict(), \n                    }, 'checpoint_epoch_'+str(epoch)+'.pt')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:35:52.479426Z","iopub.execute_input":"2021-08-22T20:35:52.479811Z","iopub.status.idle":"2021-08-22T20:35:52.495214Z","shell.execute_reply.started":"2021-08-22T20:35:52.479762Z","shell.execute_reply":"2021-08-22T20:35:52.49307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            y = y.to(device)\n            output = model(x)\n            _, predictions = torch.max(output, 1)\n            correct += (predictions == y).sum().item()\n            test_loss = criterion(output, y)\n            \n    test_loss /= len(test_loader.dataset)\n    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n    len(test_loader.dataset), \"  \", int(correct / len(test_loader.dataset) * 100), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train(5, model) # train\n    #test() # test","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:36:00.883219Z","iopub.execute_input":"2021-08-22T20:36:00.883556Z","iopub.status.idle":"2021-08-22T20:40:46.877587Z","shell.execute_reply.started":"2021-08-22T20:36:00.883524Z","shell.execute_reply":"2021-08-22T20:40:46.876805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"----> Loading checkpoint\")\ncheckpoint = torch.load(\"./checpoint_epoch_4.pt\") # Try to load last checkpoint\nmodel.load_state_dict(checkpoint[\"model_state_dict\"]) \noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Selector Network","metadata":{}},{"cell_type":"markdown","source":"**Patching image ðŸ‘€**","metadata":{}},{"cell_type":"code","source":"!pip install patchify\nfrom patchify import patchify, unpatchify\n#patches = patchify(image, (16, 16), step=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:40:55.595508Z","iopub.execute_input":"2021-08-22T20:40:55.595864Z","iopub.status.idle":"2021-08-22T20:41:03.891982Z","shell.execute_reply.started":"2021-08-22T20:40:55.595827Z","shell.execute_reply":"2021-08-22T20:41:03.891114Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**positional encodingðŸŽˆ**","metadata":{}},{"cell_type":"code","source":"def positionalencoding1d(d_model, length):\n    \"\"\"\n    :param d_model: dimension of the model\n    :param length: length of positions\n    :return: length*d_model position matrix\n    \"\"\"\n    if d_model % 2 != 0:\n        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n                         \"odd dim (got dim={:d})\".format(d_model))\n    pe = torch.zeros(length, d_model)\n    position = torch.arange(0, length).unsqueeze(1)\n    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n                         -(math.log(10000.0) / d_model)))\n    pe[:, 0::2] = torch.sin(position.float() * div_term)\n    pe[:, 1::2] = torch.cos(position.float() * div_term)\n\n    return pe","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:41:10.684932Z","iopub.execute_input":"2021-08-22T20:41:10.68534Z","iopub.status.idle":"2021-08-22T20:41:10.694943Z","shell.execute_reply.started":"2021-08-22T20:41:10.685301Z","shell.execute_reply":"2021-08-22T20:41:10.693758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main ClassðŸ§¨**","metadata":{}},{"cell_type":"code","source":"class attention_sellector(nn.Module):\n    \n    def __init__(self, patch_size = 16, d_model=256, attention_head=8):\n        super(attention_sellector, self).__init__()\n        self.device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.patch_size    = patch_size\n        self.d_model       = d_model\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=attention_head)\n        self.transformer   = nn.TransformerEncoder(self.encoder_layer,6)\n        self.fc            = nn.Linear(self.patch_size ** 2, self.d_model)\n        self.softmax       = nn.Softmax(dim=2)\n    \n    def add_positionalencoding(self,x, input_shape):\n        # Assuming all the images are Square and height and width are equal\n        # x -> (batch, tokens, d_model)\n        # pe -> (tokens , d_model)\n        pe  = positionalencoding1d(self.d_model, (input_shape // self.patch_size)**2)\n        pe  = pe.to(self.device)\n        return torch.add(x,pe)\n    \n    def patchify_image(self,imgs, input_shape=224):\n        imgs       = torch.reshape(imgs,(-1, input_shape, input_shape))\n        img_copy   = imgs.cpu().numpy()\n        \n        patches    = patchify(img_copy, (1, self.patch_size, self.patch_size), step=(1,self.patch_size, self.patch_size))\n        batch_size = imgs.size()[0]\n        patches    = np.reshape(patches, (batch_size,-1,self.patch_size,self.patch_size))\n        out        = torch.from_numpy(patches) # out == (batch_size, 196, 16 , 16) \n        out        = out.to(device=self.device)\n        return  out\n     \n    def unpatchify(self, input_tensor, input_shape=224): #out == (batch_size, 196, 256)\n        out = torch.reshape(input_tensor, (input_tensor.size()[0], input_shape // self.patch_size, input_shape // self.patch_size, self.patch_size, self.patch_size))\n        # out == (batch_size,14,14,16,16)\n        out_copy = out.numpy()\n        out_copy = unpatchify(patches, (input_tensor.size()[0], input_shape, input_shape))\n        out      = torch.from_numpy(out_copy) #out == (batch_size, 224, 224)\n        return out\n    \n    def torch_delete(self,tensor, indices):\n        mask = torch.ones(tensor.numel(), dtype=torch.bool)\n        mask[indices] = False\n        return tensor[mask]\n    \n    \n    def select_attention(self,img, attention, attention_num = 20000):  #attention -> (batch_size, 196, 256)\n                                                                       #img       -> (batch_size, 224, 224)\n        attention_copy = torch.reshape(attention, (attention.size()[0], 1, -1)) #Attention_copy -> (batch_size, 1, 196*256)\n        img_copy       = torch.reshape(img,       (img.size()[0],1,-1))\n        for i in range(attention_copy.size()[0]):\n            idx                      = torch.multinomial(attention_copy[i,:,:], attention_num)\n            idx, _                   = torch.sort(idx)\n            idx_delete               = self.torch_delete(torch.arange(196*256),idx)   #correct these numbers\n            img_copy[i,:,idx_delete] = 0\n            temp                     = torch.reshape(img_copy[i,:,:],(1,img.size()[2],img.size()[3])) #temp -> (1,224, 224) !!!\n            if i == 0:\n                out                  = temp\n            else:\n                out                  = torch.cat((out,temp),0)\n        return out\n     \n     \n    def apply_linear(self, input_tensor): # input == (batch_size, 196, 16 , 16)\n        #add resnet layer later\n        out = torch.flatten(input_tensor, start_dim=2) # (batch_size, 196, 256)\n        out = self.fc(out) # (batch_size, 196, 256)\n        return out\n    \n    def forward(self, x): #x -> (batch_size, 1, 224, 224)\n        \n        out = self.patchify_image(x)  #out == (batch_size, 196, 16 , 16) \n        out = self.apply_linear(out)  #out == (batch_size, 196, 256)\n        out = self.add_positionalencoding(out, 224) #out == (batch_size, 196, 256)\n        out = self.transformer(out)   #out == (batch_size, 196, 256)\n        out = self.softmax(out)       #out == (batch_size, 196, 256)\n        out = self.select_attention(x,out)  #out == (batch_size, 224, 224)\n        out = torch.reshape(out, (-1,1,224 ,224))     #correct numbers\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:41:17.354187Z","iopub.execute_input":"2021-08-22T20:41:17.354511Z","iopub.status.idle":"2021-08-22T20:41:17.382373Z","shell.execute_reply.started":"2021-08-22T20:41:17.35448Z","shell.execute_reply":"2021-08-22T20:41:17.381238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training this amazing modelðŸ•¶**","metadata":{}},{"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\nattention_model = attention_sellector()\nattention_model.to(attention_model.device)\n\ndef train_attention(num_epoch, attention_model, detection_model):\n    for epoch in range(0, num_epoch):\n\n        losses = []\n        attention_model.train()\n        detection_model.eval()\n        optimizer = optim.Adam(attention_model.parameters(), lr=0.001)\n        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n        for batch_idx, (data, targets) in loop:\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            attention_output = attention_model(data)\n            scores           = detection_model(attention_output)\n            \n            loss = criterion(scores, targets)\n            optimizer.zero_grad()\n            losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            _, preds = torch.max(scores, 1)\n            \n            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n            loop.set_postfix(loss=loss.data.item())\n        \n        # save model\n        torch.save({ \n                    'model_state_dict': attention_model.state_dict(), \n                    'optimizer_state_dict': optimizer.state_dict(), \n                    }, 'checpoint_epoch_'+str(epoch)+'.pt')\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:41:33.082532Z","iopub.execute_input":"2021-08-22T20:41:33.082916Z","iopub.status.idle":"2021-08-22T20:41:33.146572Z","shell.execute_reply.started":"2021-08-22T20:41:33.082881Z","shell.execute_reply":"2021-08-22T20:41:33.145604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_detection(num_epoch, attention_model, detection_model):\n    for epoch in range(0, num_epoch):\n\n        losses = []\n        attention_model.eval()\n        detection_model.train()\n        optimizer = optim.Adam(detection_model.parameters(), lr=0.001)\n        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n        for batch_idx, (data, targets) in loop:\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            attention_output = attention_model(data)\n            scores           = detection_model(attention_output)\n            \n            loss = criterion(scores, targets)\n            optimizer.zero_grad()\n            losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            _, preds = torch.max(scores, 1)\n            \n            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n            loop.set_postfix(loss=loss.data.item())\n        \n        # save model\n        torch.save({ \n                    'model_state_dict': detection_model.state_dict(), \n                    'optimizer_state_dict': optimizer.state_dict(), \n                    }, 'checpoint_epoch_'+str(epoch)+'.pt')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:49:26.155711Z","iopub.execute_input":"2021-08-22T20:49:26.156086Z","iopub.status.idle":"2021-08-22T20:49:26.169727Z","shell.execute_reply.started":"2021-08-22T20:49:26.156054Z","shell.execute_reply":"2021-08-22T20:49:26.169023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    torch.cuda.empty_cache()\n    train_detection(5,attention_model,model)\n    train_attention(5, attention_model, model) # train","metadata":{"execution":{"iopub.status.busy":"2021-08-22T20:49:34.686692Z","iopub.execute_input":"2021-08-22T20:49:34.687028Z","iopub.status.idle":"2021-08-22T21:02:15.333882Z","shell.execute_reply.started":"2021-08-22T20:49:34.686997Z","shell.execute_reply":"2021-08-22T21:02:15.332933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(attention_model, detection_model):\n    attention_model.eval()\n    detection_model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            y = y.to(device)\n            x = attention_model(x)\n            output = detection_model(x)\n            _, predictions = torch.max(output, 1)\n            correct += (predictions == y).sum().item()\n            test_loss = criterion(output, y)\n            \n    test_loss /= len(test_loader.dataset)\n    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n    len(test_loader.dataset), \"  \", int(correct / len(test_loader.dataset) * 100), \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-08-22T21:05:34.935929Z","iopub.execute_input":"2021-08-22T21:05:34.93632Z","iopub.status.idle":"2021-08-22T21:05:34.947069Z","shell.execute_reply.started":"2021-08-22T21:05:34.936283Z","shell.execute_reply":"2021-08-22T21:05:34.94557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(attention_model,model)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T21:06:05.140201Z","iopub.execute_input":"2021-08-22T21:06:05.140515Z","iopub.status.idle":"2021-08-22T21:06:18.818717Z","shell.execute_reply.started":"2021-08-22T21:06:05.140483Z","shell.execute_reply":"2021-08-22T21:06:18.817881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the test set\ntest_dataset = ImageFolder(\"../input/cat-and-dog/test_set/test_set/\", \n                     transform=transforms.Compose([\n                         transforms.Grayscale(num_output_channels=1),\n                         transforms.Resize((224, 224)), \n                         transforms.ToTensor()]))\n                         #transforms.Normalize([0]*1, [1]*1)\n                     \ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T21:33:57.64678Z","iopub.execute_input":"2021-08-22T21:33:57.647122Z","iopub.status.idle":"2021-08-22T21:33:57.671209Z","shell.execute_reply.started":"2021-08-22T21:33:57.647091Z","shell.execute_reply":"2021-08-22T21:33:57.670336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nwith torch.no_grad():\n    attention_model.eval()\n    i = 0\n    for data, target in test_dataloader:\n        if i > 3: break\n        i +=1\n        data, target = data.to(device), target.to(device)\n        output = attention_model(data)\n        data = torch.reshape(data,(224,224))\n        output = torch.reshape(output,(224,224))\n        plt.imshow(output.cpu().numpy(), cmap='gray')\n        plt.show()\n        plt.imshow(data.cpu().numpy(), cmap='gray')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T21:34:21.713788Z","iopub.execute_input":"2021-08-22T21:34:21.714149Z","iopub.status.idle":"2021-08-22T21:34:22.969249Z","shell.execute_reply.started":"2021-08-22T21:34:21.714113Z","shell.execute_reply":"2021-08-22T21:34:22.968518Z"},"trusted":true},"execution_count":null,"outputs":[]}]}